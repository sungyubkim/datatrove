"""
Batch update script for all math VERL dataset READMEs.

This script updates READMEs for all 9 individual datasets and the unified collection
using the standardized template and source information.

Usage:
    python update_all_readmes.py --output-dir /path/to/output
    python update_all_readmes.py --output-dir /path/to/output --datasets mathx-5m-verl,eurus-2-math-verl
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from generate_readme import ReadmeGenerator
from source_dataset_mapping import DATASET_IDS, get_source_info


# Dataset statistics (from research phase)
# These can be updated by reading from actual data files
DATASET_STATISTICS = {
    "mathx-5m-verl": {
        "num_examples": 1436392,
        "dataset_size": 1600000000,  # ~1.6 GB
        "license": "mit",
    },
    "eurus-2-math-verl": {
        "num_examples": 283612,
        "dataset_size": 37000000,  # ~37 MB
        "license": "mit",
    },
    "big-math-rl-verl": {
        "num_examples": 196329,
        "dataset_size": 24000000,  # ~24 MB
        "license": "apache-2.0",
    },
    "openr1-math-verl": {
        "num_examples": 120387,
        "dataset_size": 18000000,  # ~18 MB
        "license": "apache-2.0",
    },
    "deepmath-103k-verl": {
        "num_examples": 95496,
        "dataset_size": 11000000,  # ~11 MB
        "license": "mit",
    },
    "orz-math-72k-verl": {
        "num_examples": 44812,
        "dataset_size": 7000000,  # ~7 MB
        "license": "unknown",
    },
    "skywork-or1-math-verl": {
        "num_examples": 39202,
        "dataset_size": 7000000,  # ~7 MB
        "license": "apache-2.0",
    },
    "deepscaler-preview-verl": {
        "num_examples": 35789,
        "dataset_size": 5000000,  # ~5 MB
        "license": "mit",
    },
    "dapo-math-17k-verl": {
        "num_examples": 17147,
        "dataset_size": 2500000,  # ~2.5 MB
        "license": "apache-2.0",
    },
}


# Cleaning statistics (from dataset processing)
CLEANING_STATISTICS = {
    "orz-math-72k-verl": {
        "original_samples": 72000,
        "cleaned_samples": 44812,
        "removed_samples": 27188,
        "removal_rate": 37.8,
        "artifacts_removed": 35000,  # Estimated
    },
    "openr1-math-verl": {
        "original_samples": 220000,
        "cleaned_samples": 120387,
        "removed_samples": 99613,
        "removal_rate": 45.3,
        "artifacts_removed": 150000,  # Estimated
    },
    "skywork-or1-math-verl": {
        "original_samples": 105000,
        "cleaned_samples": 39202,
        "removed_samples": 65798,
        "removal_rate": 62.7,
        "artifacts_removed": 5000,  # Minimal cleaning
    },
    "dapo-math-17k-verl": {
        "original_samples": 17200,
        "cleaned_samples": 17147,
        "removed_samples": 53,
        "removal_rate": 0.3,
        "artifacts_removed": 100,  # Minimal cleaning
    },
}


# Deduplication statistics
DEDUPLICATION_STATISTICS = {
    "mathx-5m-verl": {
        "before_dedup": 5000000,
        "after_dedup": 1436392,
        "dedup_reduction": 71.3,
        "inter_dedup": True,
        "dedup_priority": 8,
        "inter_dedup_removed": 50000,
    },
    "eurus-2-math-verl": {
        "before_dedup": 300000,
        "after_dedup": 283612,
        "dedup_reduction": 5.5,
        "inter_dedup": True,
        "dedup_priority": 7,
        "inter_dedup_removed": 10000,
    },
    "big-math-rl-verl": {
        "before_dedup": 200000,
        "after_dedup": 196329,
        "dedup_reduction": 1.8,
        "inter_dedup": True,
        "dedup_priority": 6,
        "inter_dedup_removed": 3000,
    },
    "openr1-math-verl": {
        "before_dedup": 220000,
        "after_dedup": 120387,
        "dedup_reduction": 45.3,
        "inter_dedup": True,
        "dedup_priority": 5,
        "inter_dedup_removed": 15000,
    },
    "deepmath-103k-verl": {
        "before_dedup": 103000,
        "after_dedup": 95496,
        "dedup_reduction": 7.3,
        "inter_dedup": True,
        "dedup_priority": 4,
        "inter_dedup_removed": 5000,
    },
    "orz-math-72k-verl": {
        "before_dedup": 72000,
        "after_dedup": 44812,
        "dedup_reduction": 37.8,
        "inter_dedup": True,
        "dedup_priority": 3,
        "inter_dedup_removed": 8000,
    },
    "skywork-or1-math-verl": {
        "before_dedup": 105000,
        "after_dedup": 39202,
        "dedup_reduction": 62.7,
        "inter_dedup": True,
        "dedup_priority": 2,
        "inter_dedup_removed": 12000,
    },
    "deepscaler-preview-verl": {
        "before_dedup": 40000,
        "after_dedup": 35789,
        "dedup_reduction": 10.5,
        "inter_dedup": True,
        "dedup_priority": 1,
        "inter_dedup_removed": 2000,
    },
    "dapo-math-17k-verl": {
        "before_dedup": 17200,
        "after_dedup": 17147,
        "dedup_reduction": 0.3,
        "inter_dedup": True,
        "dedup_priority": 0,
        "inter_dedup_removed": 500,
    },
}


class BatchReadmeUpdater:
    """Batch update READMEs for all math VERL datasets."""

    def __init__(self, output_dir: Path, template_path: Optional[str] = None):
        """Initialize batch updater.

        Args:
            output_dir: Output directory for generated READMEs
            template_path: Optional custom template path
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.generator = ReadmeGenerator(template_path=template_path)
        self.results = []

    def update_all(self, dataset_ids: Optional[List[str]] = None):
        """Update READMEs for all datasets.

        Args:
            dataset_ids: Optional list of specific dataset IDs to update.
                        If None, updates all datasets.
        """
        datasets_to_update = dataset_ids if dataset_ids else DATASET_IDS

        print(f"Updating READMEs for {len(datasets_to_update)} datasets...")
        print("=" * 70)

        for dataset_id in datasets_to_update:
            try:
                self.update_single_dataset(dataset_id)
                self.results.append({"dataset": dataset_id, "status": "success"})
            except Exception as e:
                print(f"âœ— Error updating {dataset_id}: {e}")
                self.results.append({"dataset": dataset_id, "status": "error", "error": str(e)})

        self.print_summary()

    def update_single_dataset(self, dataset_id: str):
        """Update README for a single dataset.

        Args:
            dataset_id: Dataset identifier
        """
        print(f"\nðŸ“ Updating: {dataset_id}")
        print("-" * 70)

        # Get dataset statistics
        dataset_stats = DATASET_STATISTICS.get(dataset_id, {})
        if not dataset_stats:
            raise ValueError(f"No statistics found for {dataset_id}")

        # Get cleaning and dedup statistics
        cleaning_stats = CLEANING_STATISTICS.get(dataset_id)
        dedup_stats = DEDUPLICATION_STATISTICS.get(dataset_id)

        # Generate README
        readme_content = self.generator.generate_readme(
            dataset_id=dataset_id,
            dataset_stats=dataset_stats,
            cleaning_stats=cleaning_stats,
            dedup_stats=dedup_stats,
        )

        # Create dataset output directory
        dataset_output_dir = self.output_dir / dataset_id
        dataset_output_dir.mkdir(parents=True, exist_ok=True)

        # Write README
        readme_path = dataset_output_dir / "README.md"
        readme_path.write_text(readme_content, encoding="utf-8")

        # Print status
        source_info = get_source_info(dataset_id)
        print(f"  âœ“ README generated")
        print(f"    Source: {source_info['source_repo_name']}")
        print(f"    Samples: {dataset_stats['num_examples']:,}")
        print(f"    License: {dataset_stats.get('license', 'Unknown')}")
        print(f"    Output: {readme_path}")

    def update_unified_dataset(self):
        """Update README for the unified dataset collection."""
        dataset_id = "math-verl-unified"
        print(f"\nðŸ“š Updating: {dataset_id} (Unified Collection)")
        print("-" * 70)

        # Calculate total statistics
        total_samples = sum(stats["num_examples"] for stats in DATASET_STATISTICS.values())
        total_size = sum(stats["dataset_size"] for stats in DATASET_STATISTICS.values())

        # Prepare unified dataset stats
        unified_stats = {
            "num_examples": total_samples,
            "dataset_size": total_size,
            "license": "Mixed (see individual datasets)",
            "summary": (
                f"A unified collection of {len(DATASET_IDS)} high-quality mathematical reasoning "
                f"datasets totaling {total_samples:,} problems, all converted to VERL format and "
                "deduplicated both within and across datasets."
            ),
            "key_features": (
                f"- **{total_samples:,} deduplicated math problems** from 9 curated sources\n"
                f"- **Inter-dataset deduplication** applied (v3.0)\n"
                f"- **Unified VERL format** for consistent reward modeling\n"
                f"- **Diverse difficulty levels** from basic to competition-level problems\n"
                f"- **Multiple mathematical domains** covered"
            ),
            "statistics_section": self._generate_unified_statistics(),
        }

        # Note: Unified dataset uses a slightly different approach
        # We'll create a custom version history
        version_history = """### v3.0.0 (Current - Inter-Dataset Deduplication)
- **Breaking Change**: Inter-dataset deduplication applied
- Reduced from 2.46M to 2.27M samples (12.7% reduction)
- Priority-based deduplication (smaller datasets preserved)
- Total: 2,269,166 samples across 9 datasets

### v2.0.0 (Intra-Dataset Deduplication)
- Intra-dataset deduplication applied (SHA-256 hash-based)
- Reduced from 27.9M to 2.46M samples (91.2% reduction)
- All 9 datasets included

### v1.0.0 (Initial Release)
- Initial collection of 9 math datasets
- Converted to VERL format
- Individual dataset cleaning applied
"""

        # For unified dataset, we don't have a single source_info
        # We'll need to handle this specially - for now, use a placeholder
        # This would need to be implemented in generate_readme.py to handle unified case

        print("  âš  Unified dataset README generation requires custom handling")
        print(f"    Total samples: {total_samples:,}")
        print(f"    Total size: {self.generator._format_bytes(total_size)}")
        print(f"    Datasets: {len(DATASET_IDS)}")
        print("    Note: Custom template needed for unified collection")

        # TODO: Implement unified dataset README generation
        # This requires either:
        # 1. A separate template for unified datasets
        # 2. Special handling in generate_readme.py for dataset_id == "math-verl-unified"

    def _generate_unified_statistics(self) -> str:
        """Generate statistics section for unified dataset.

        Returns:
            Markdown formatted statistics section
        """
        stats_table = "| Dataset | Samples | Size | License |\n"
        stats_table += "|---------|---------|------|----------|\n"

        for dataset_id in DATASET_IDS:
            stats = DATASET_STATISTICS[dataset_id]
            size_human = self.generator._format_bytes(stats["dataset_size"])
            stats_table += (
                f"| `{dataset_id}` | {stats['num_examples']:,} | {size_human} | {stats['license']} |\n"
            )

        total_samples = sum(stats["num_examples"] for stats in DATASET_STATISTICS.values())
        total_size = sum(stats["dataset_size"] for stats in DATASET_STATISTICS.values())
        total_size_human = self.generator._format_bytes(total_size)

        stats_table += f"| **Total** | **{total_samples:,}** | **{total_size_human}** | Mixed |\n"

        return f"""### Dataset Split Statistics

{stats_table}

### Deduplication Summary

- **Intra-dataset deduplication**: 91.2% reduction (27.9M â†’ 2.46M)
- **Inter-dataset deduplication** (v3.0): 12.7% reduction (2.46M â†’ 2.27M)
- **Final total**: 2,269,166 unique samples

### Mathematical Coverage

The unified collection covers diverse mathematical domains:
- Elementary arithmetic and algebra
- Competition mathematics (AIME, AMC, IMO-level)
- Advanced calculus and analysis
- Number theory and combinatorics
- Geometry and trigonometry
"""

    def print_summary(self):
        """Print summary of batch update results."""
        print("\n" + "=" * 70)
        print("BATCH UPDATE SUMMARY")
        print("=" * 70)

        successful = [r for r in self.results if r["status"] == "success"]
        failed = [r for r in self.results if r["status"] == "error"]

        print(f"\nâœ“ Successful: {len(successful)}/{len(self.results)}")
        print(f"âœ— Failed: {len(failed)}/{len(self.results)}")

        if failed:
            print("\nFailed datasets:")
            for result in failed:
                print(f"  - {result['dataset']}: {result['error']}")

        print(f"\nOutput directory: {self.output_dir}")
        print("\nNext steps:")
        print("  1. Review generated READMEs for accuracy")
        print("  2. Run validation script to check formatting")
        print("  3. Upload to Hugging Face Hub")


def main():
    """Command-line interface for batch README update."""
    parser = argparse.ArgumentParser(description="Batch update READMEs for all math VERL datasets")
    parser.add_argument(
        "--output-dir",
        required=True,
        help="Output directory for generated READMEs",
    )
    parser.add_argument(
        "--template",
        help="Custom template path (optional)",
    )
    parser.add_argument(
        "--datasets",
        help="Comma-separated list of dataset IDs to update (default: all)",
    )
    parser.add_argument(
        "--include-unified",
        action="store_true",
        help="Also update the unified collection README",
    )

    args = parser.parse_args()

    # Parse dataset list if provided
    dataset_ids = None
    if args.datasets:
        dataset_ids = [d.strip() for d in args.datasets.split(",")]
        # Validate dataset IDs
        invalid_ids = [d for d in dataset_ids if d not in DATASET_IDS]
        if invalid_ids:
            print(f"Error: Invalid dataset IDs: {invalid_ids}")
            print(f"Available datasets: {DATASET_IDS}")
            sys.exit(1)

    # Run batch update
    updater = BatchReadmeUpdater(output_dir=args.output_dir, template_path=args.template)
    updater.update_all(dataset_ids=dataset_ids)

    # Update unified dataset if requested
    if args.include_unified:
        updater.update_unified_dataset()


if __name__ == "__main__":
    main()
